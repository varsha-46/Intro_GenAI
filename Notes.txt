Basically, GenAI uses the Foundational models which means it can solve a coding question, rephrases the sentences, logical query etc. Previous ML models used works on single task only like prediction and classify etc. 
When it comes to GENAI it has huge architecture and lot of parameters need lot of GPU for training(similarly it has whole internet data). Foundational models are LLM, Large Multi-Models. 
When comes to builder's perspective we have to learn:
-->Transformers Architecture
-->Encoder-Only(BERT), Decoder-Only(GPT) and Encoder-decoder(T5).
-->Pre Training
-->Optimization
-->Fine Tuning
-->Evaluation 
-->Deployment
When comes to User's perspective:
-->Building LLMs and making it use using Huggingface Transformers, Ollama, Open/Closed source API etc
-->Prompt Engineering
-->RAG
-->Fine Tuning
-->Agents
-->LLMOps
---------------------------------------------------------------------------------------------------------------------------------------------------------------------
LangChain:
It is Open-source framework that helps in building LLM based applications. provides modular componennts and end-to-end tools that help developer to build complex AI applications such as chatbots, qna syst ems, RAG, autonomous agents etc.
>Supports all major LLMs, simiply developing LLM based applications, integration available, opensource/free, supports all major GenAI use cases.

***LangChain***
Open-source framework for developing applications powered by LLMs. 
----------------------------------------------------------------------------------------------------------
Components of LangChain - 6 components(Models,
Models - core interfaces through which you interact with AI models.
In general the size of models will be around 100gb, any single user cannot have that much gb on their system. So, companies like openai, google have deployed in their servers and using api calls for the user to make their query run on their servers.
Suppose, if you want to use different model for different usecase previously the codes will be completely different then LangChain comes to picture. We can use one or more models in single code itself.
Langchain uses two models: Language model(LLMs have text as input and output and use agents) and Embedding model(give text as input getting output as vectors for sematic search)
Prompts - Prompt is the text which we used to pass to a model and we have dynamic prompt in Langchain which we can pass empotion,query whcih will be Dynamic & reusable, Role - based and few-shot prompting to provide similar output. 
Chains - These are the important features for a langchain, their will be prompt will pass to LLM and then to create report and it will be passed to some other process instead the langchains have some functions to do it.
Indexs - Suppose chatGPT wont have internal data like company policies so we need to use internal pdfs or data to get the answers. Here the main components will be Doc Loader. Text spiltter,vector store, retrivors 
Memory - These LLM model's memory is stateless which means that it wont store the context of the chat but their are some methods to include.
Agents - Agents are the going norm which we are using the role of an agent is to link your model with other APIs as well such as weatherAPI etc
-------------------------------------------------------------------------------------------------




